# [Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/abs/2309.06180)

## é¢„å¤‡çŸ¥è¯†

### transformerçš„self-attention layers

> For an input hidden state sequence $(ð‘¥1, . . . , ð‘¥ð‘›) âˆˆ R^{ð‘›Ã—ð‘‘}$ , a self-attention layer first applies linear transformations on each position ð‘– to get the query, key, and value vectors:
>
> ![image-20240331111833960](/images/llm-4/5)
>
> Then, the self-attention layer computes the attention score $ð‘Ž_{ð‘– ð‘—}$ by multiplying the query vector at one position with all the key vectors before it and compute the output $ð‘œ_ð‘–$ as the weighted average over the value vectors:
>
> ![image-20240331111955260](/images/llm-4/6)

ç®€è€Œè¨€ä¹‹ï¼Œå…ˆæŠŠæ¯ä¸ªä½ç½®çš„è¯ç®—å‡ºå…¶q k vï¼Œç„¶åŽç”¨æ³¨æ„åŠ›å…¬å¼ç®—å‡ºæ¯ä¸¤ä¸ªè¯ä¹‹é—´çš„aå’Œæ¯ä¸ªè¯çš„oã€‚

é™¤æ­¤ä¹‹å¤–ï¼Œtransformer including the embedding layer, feed-forward layer, layer normalization, residual connection, output logit computation, and the query, key, and value transformation.

### KV Cache

> ä»¥GPTä¸ºä»£è¡¨çš„Decoder-Onlyè‡ªå›žå½’è¯­è¨€æ¨¡åž‹åœ¨ç”Ÿæˆæ¯ä¸€ä¸ªæ–°çš„ token æ—¶ï¼ŒæŽ¥å—æ‰€æœ‰ä¹‹å‰ç”Ÿæˆçš„ tokens ä½œä¸ºè¾“å…¥ã€‚ç„¶è€Œï¼Œå¯¹äºŽè¿™äº›å…ˆå‰ç”Ÿæˆçš„ tokensï¼Œæ¯æ¬¡ç”Ÿæˆæ–°çš„ token æ—¶éƒ½éœ€è¦é‡æ–°è®¡ç®—ä»–ä»¬çš„è¡¨ç¤ºï¼Œè¿™ä¸ªè¿‡ç¨‹é€ æˆäº†å¤§é‡çš„è®¡ç®—æµªè´¹ã€‚KV Cache çš„å¼•å…¥å°±æ˜¯ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ã€‚
>
> KV Cacheå®žè´¨ä¸Šæ˜¯å­˜å‚¨äº†ä¹‹å‰è®¡ç®—è¿‡çš„ key-value å¯¹ç”¨äºŽä¸‹ä¸€ä¸ªTokençš„ç”Ÿæˆã€‚åœ¨ Transformer ç»“æž„ä¸­ï¼Œself-attention ä¸­çš„k_proj, v_projä¼šå°†è¾“å…¥çš„æ¯ä¸ª token è½¬åŒ–ä¸ºä¸€ä¸ª key å’Œä¸€ä¸ª valueï¼Œç„¶åŽä½¿ç”¨è¿™äº› key-value ä»¥åŠå½“å‰çš„queryå¯¹æ¥è®¡ç®—ä¸‹ä¸€ä¸ª tokenã€‚å¼•å…¥ KV Cacheï¼Œæˆ‘ä»¬å°±å¯ä»¥å°†ä¹‹å‰ç”Ÿæˆçš„ tokens å¯¹åº”çš„ key-value å¯¹å­˜å‚¨èµ·æ¥ï¼Œå½“ç”Ÿæˆæ–°çš„ token æ—¶ï¼Œ**ç›´æŽ¥ä»Ž KV Cache ä¸­å–å‡ºè¿™äº›å·²ç»è®¡ç®—å¥½çš„ key-value å¯¹ï¼Œå†æŠŠå½“å‰tokençš„key-valueåšä¸€ä¸ªè¿žç»“åœ¨è¿›è¡Œè®¡ç®—**ï¼Œè¿™æ ·å°±é¿å…äº†KVçš„é‡å¤è®¡ç®—ï¼Œå¤§å¤§æé«˜äº†è®¡ç®—æ•ˆçŽ‡ã€‚

KV CacheåŒ…å«ä»¥ä¸‹æ­¥éª¤

> **é¢„å¡«å……é˜¶æ®µ**ï¼šåœ¨è®¡ç®—ç¬¬ä¸€ä¸ªè¾“å‡ºtokenè¿‡ç¨‹ä¸­ï¼Œæ­¤æ—¶Cacheæ˜¯ç©ºçš„ï¼Œè®¡ç®—æ—¶éœ€è¦ä¸ºæ¯ä¸ª transformer layer è®¡ç®—å¹¶ä¿å­˜key cacheå’Œvalue cacheï¼Œåœ¨è¾“å‡ºtokenæ—¶Cacheå®Œæˆå¡«å……ï¼›FLOPsåŒKV Cacheå…³é—­ä¸€è‡´ï¼Œå­˜åœ¨å¤§é‡gemmæ“ä½œï¼ŒæŽ¨ç†é€Ÿåº¦æ…¢ï¼Œè¿™æ—¶å±žäºŽCompute-boundç±»åž‹è®¡ç®—ã€‚
>
> **KV Cacheé˜¶æ®µ**ï¼šåœ¨è®¡ç®—ç¬¬äºŒä¸ªè¾“å‡ºtokenè‡³æœ€åŽä¸€ä¸ªtokenè¿‡ç¨‹ä¸­ï¼Œæ­¤æ—¶Cacheæ˜¯æœ‰å€¼çš„ï¼Œæ¯è½®æŽ¨ç†åªéœ€è¯»å–Cacheï¼ŒåŒæ—¶å°†å½“å‰è½®è®¡ç®—å‡ºçš„æ–°çš„Keyã€Valueè¿½åŠ å†™å…¥è‡³Cacheï¼›FLOPsé™ä½Žï¼Œgemmå˜ä¸ºgemvæ“ä½œï¼ŒæŽ¨ç†é€Ÿåº¦ç›¸å¯¹ç¬¬ä¸€é˜¶æ®µå˜å¿«ï¼Œè¿™æ—¶å±žäºŽMemory-boundç±»åž‹è®¡ç®—ã€‚

> æ— KV Cacheç”Ÿæˆç¤ºä¾‹
>
> ![image-20240330213245722](/images/llm-4/3)

> KV Cacheç”Ÿæˆç¤ºä¾‹
>
> ![image-20240330213306988](/images/llm-4/4)

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œä½¿ç”¨äº†KV CacheèŠ‚çœäº†å¤§é‡çš„é‡å¤è®¡ç®—ã€‚

### å½“å‰Batching Techniques for LLMs

å›°å¢ƒï¼š

* è¯·æ±‚å¯èƒ½åœ¨ä¸åŒæ—¶é—´æ®µåˆ°è¾¾
* è¯·æ±‚åºåˆ—çš„é•¿åº¦ä¸åŒ

ç›®å‰æ–¹æ³•ï¼š

* é‡‡ç”¨ç»†ç²’åº¦çš„æ‰¹å¤„ç†æœºåˆ¶

### Memory Challenges in LLM Serving

æŒ‘æˆ˜ï¼š

* KV cacheå¤ªå¤§äº†ï¼Œä¸”GPUçš„è®¡ç®—èƒ½åŠ›ä¼šæ¯”å…¶å†…å­˜å¢žé•¿å¾—æ›´å¿«ã€‚
* decodingç®—æ³•è¶Šæ¥è¶Šå¤æ‚ï¼Œå¦‚ä½•é€‚é…ã€‚
* è¾“å…¥è¾“å‡ºé•¿åº¦ä¸åŒçš„æƒ…å†µä¸‹å¦‚ä½•è°ƒåº¦èµ„æºã€‚

## vLLM

### æ•ˆæžœ

**æ•ˆæžœ**ï¼š

>  PagedAttention, an attention algorithm inspired by the classical **virtual memory** and **paging techniques** in operating systems

æƒ³æ³•æ¥æºäºŽè™šæ‹Ÿå†…å­˜å’Œåˆ†é¡µæŠ€æœ¯

> vLLM, an LLM serving system that achieves (1) **near-zero waste** in KV cache memory and (2) **flexible sharing of KV cache** within and across requests to further reduce memory usage.

> ![image-20240330194543256](/images/llm-4/1)

å·¦å›¾å°±æ˜¯vLLMå°†KV CacheæŽ§åˆ¶åœ¨çº¢è‰²ï¼Œä¸”ç”¨ä¸€éƒ¨åˆ†é»„è‰²è¿›è¡Œæ¿€æ´»ã€‚æ‰€ä»¥éšç€è§„æ¨¡æ‰©å¤§ï¼ŒvLLMçš„å†…å­˜ä½¿ç”¨é‡å¯ä»¥æŽ§åˆ¶å¾—æ›´å¥½ã€‚æ­£å¦‚å³å›¾æ‰€ç¤ºã€‚

> ![image-20240330194852044](/images/llm-4/2)

å†…å­˜èµ„æºæµªè´¹å¹³å‡ç™¾åˆ†æ¯”å›¾ï¼Œå¯ä»¥çœ‹åˆ°vLLMçš„æœ‰æ•ˆæ€§ã€‚

### PagedAttention

**ç‰¹è‰²ï¼š**å…è®¸ä¸è¿žç»­çš„KV cacheå­˜å‚¨æ–¹å¼ã€‚æ–¹æ³•æ˜¯é‡‡ç”¨åˆ†å—çš„æ–¹å¼ã€‚

> ![annimation0](/images/llm-4/1.gif)

å½“åŽŸæ–‡ä¸º Alan Turing is a|computer scientist and mathmatician|renowned for ...

å…¶è¢«åˆ†æˆä¸‰ä¸ªå—æ¥å®Œæˆï¼Œè¿™ä¸‰ä¸ªå—çš„ç‰©ç†å†…å­˜ä¸ä¸€æ ·ã€‚

è®¡ç®—æ–¹å¼ä¸º

> ![image-20240331114636223](/images/llm-4/7)

è¿™æ ·å°±å¯ä»¥å‡å°‘KV Cacheçš„æµªè´¹ï¼Œæœ€å¤šæµªè´¹3ä¸ªç©ºã€‚ï¼ˆè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆæ“ä½œç³»ç»Ÿå¼•å…¥åˆ†é¡µæœºåˆ¶ï¼‰

> ![fcc6f1bb-484e-43ab-8101-dc5dbbdbcd89](/images/llm-4/2.gif)

å¹¶ä¸”æ ¹æ®æ“ä½œç³»ç»Ÿçš„**å†™æ—¶å…±äº«æœºåˆ¶**ï¼ŒPagedAttention å¯ä»¥å½“äº§ç”Ÿå¤šä¸ªç»“æžœæ—¶ï¼Œå°†ä¸‹å›¾ä¸­çš„intelligence iså¤åˆ¶åˆ°å¤šä¸ªå—ä¸Šã€‚

> ![annimation3](/images/llm-4/3.gif)

è¿˜é‡‡ç”¨äº†**æŸæœç´¢æœºåˆ¶**

> ![image-20240331120042678](/images/llm-4/8)
>
> è¿™æ˜¯ä¸€ä¸ªæŸå®½ä¸º2çš„[æŸæœç´¢æ ·ä¾‹](https://zh.d2l.ai/chapter_recurrent-modern/beam-search.html)ã€‚é€‰æ‹©æœ€å¤§çš„ä¸¤ä¸ªã€‚
>
> ![image-20240331120357189](/images/llm-4/9)
>
> åœ¨vLLMä¸­ï¼Œåˆ™æ˜¯è¿™ç§æ•ˆæžœï¼Œè·Ÿä¸Šå›¾éžå¸¸ç›¸ä¼¼ã€‚

vLLMä¹Ÿè€ƒè™‘åˆ°**å…±äº«å‰ç¼€**çš„é—®é¢˜ã€‚

> ![image-20240331120631554](/images/llm-4/10)

å¯¹äºŽæ­¤ç±»åº”ç”¨vLLMä¼šå…±äº«å‰ç¼€ï¼Œåªåœ¨Task Inputä¸Šæœ‰å·®å¼‚ã€‚å…¶å®žå°±æ˜¯å‰æ–‡çš„åˆ†é¡µæœºåˆ¶ã€‚

### Scheduling and Preemption

é‡‡ç”¨**first-come-first-serve** (**FCFS**)ï¼Œå…ˆæ¥å…ˆæœåŠ¡ã€‚

é—®é¢˜ï¼š

* å‡å¦‚æ»¡äº†ï¼Œåº”è¯¥é©±é€å“ªäº›å—
  * transformerç‰¹æ€§â†’åŒä¸€ä¸ªåºåˆ—çš„å—è¦ä¹ˆä¸€èµ·è¢«é©±é€ï¼Œè¦ä¹ˆä¸€èµ·ç•™ä¸‹ã€‚
  * å‡å¦‚æœ‰æŸæœç´¢ï¼Œå…¶å°†åºåˆ—åˆ†æˆäº†å¾ˆå¤šç»„ï¼Œä¸”å­˜åœ¨å†…å­˜å…±äº«ï¼Œç»„å†…æ‰€æœ‰åºåˆ—çš„å—åŒæ—¶è¢«è°ƒåº¦ã€‚
* å‡å¦‚ä»è¢«éœ€è¦ï¼Œå¦‚ä½•æ¢å¤è¢«é©±é€çš„å—ã€‚
  * äº¤æ¢ã€‚æ”¾åˆ°CPUå†…å­˜ä¸­ã€‚
  * é‡æ–°è®¡ç®—ã€‚å› ä¸ºè§£ç æ—¶çš„ä»¤ç‰Œå’Œç”¨æˆ·æç¤ºé“¾æŽ¥èµ·æ¥æˆä¸ºæ–°çš„æç¤ºï¼Œä¸€æ¬¡å°±å¯ä»¥ç”ŸæˆKV Cacheï¼Œæ‰€ä»¥ä¼šæ¯”ä¹‹å‰ç®—çš„å—ã€‚

### Distributed Execution

> Specifically, the attention operator is split on the attention head dimension, each SPMD process takes care of a subset of attention heads in multi-head attention.

VLLMæ˜¯å°†æ³¨æ„åŠ›ç®—å­åœ¨æ³¨æ„åŠ›å¤´ç»´åº¦ä¸Šè¿›è¡Œåˆ†å‰²ã€‚

ä¸”ç”±äºŽæ¯ä¸ªæ¨¡åž‹çš„åˆ†ç‰‡å¤„ç†ç›¸åŒçš„è¾“å…¥æ ‡è®°é›†ï¼Œæ‰€ä»¥vLLMé‡‡ç”¨çš„æ˜¯é›†ä¸­å¼è°ƒåº¦ï¼Œä¸€ä¸ªSchedulerã€‚

> This common mapping allows GPU workers to execute the model with the physical blocks provided by the scheduler for each input request. Although each GPU worker has the same physical block IDs, **a worker only stores a portion of the KV cache for its corresponding attention heads.**

ç”±äºŽå¤´ä¸åŒï¼Œç®¡ç†èµ·æ¥æ˜¯ä¸€æ ·çš„ï¼Œä½†æ˜¯æ•°æ®æ˜¯ä¸ä¸€æ ·çš„ã€‚

> ![image-20240331122203810](/images/llm-4/11)
>
> 1. In each step, the scheduler first prepares the message with input token IDs for each request in the batch, as well as the block table for each request. 
>
> 2. Next, the scheduler broadcasts this control message to the GPU workers. 
> 3. Then, the GPU workers start to execute the model with the input token IDs. 
> 4. In the attention layers, the GPU workers read the KV cache according to the block table in the control message. 
> 5. During execution, the GPU workers synchronize the intermediate results with the all-reduce communication primitive without the coordination of the scheduler, as in [47]. 
> 6. In the end, the GPU workers send the sampled tokens of this iteration back to the scheduler. 