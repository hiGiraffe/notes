Loongserve

看完

---

[Intelligent Resource Scheduling for Co-located Latency-critical Services: A Multi-Model Collaborative Learning Approach](https://arxiv.org/abs/1911.13208)

刘老师的工作，资源断崖看看有没有灵感

---

[Tabi: An Efficient Multi-Level Inference System for Large Language Models | Proceedings of the Eighteenth European Conference on Computer Systems](https://dl.acm.org/doi/10.1145/3552326.3587438)

小模型协助推理，已读，未整理

---

[[2311.15566] SpotServe: Serving Generative Large Language Models on Preemptible Instances](https://arxiv.org/abs/2311.15566)

抢占式节点上的LLM推理，可能和loongserve有协作的地方

---

Meta的两个工作

[MAST: Global Scheduling of ML Training across Geo-Distributed Datacenters at Hyperscale | USENIX](https://www.usenix.org/conference/osdi24/presentation/choudhury)

调度器的工作，听说质量非常好

[Optimizing Resource Allocation in Hyperscale Datacenters: Scalability, Usability, and Experiences | USENIX](https://www.usenix.org/conference/osdi24/presentation/kumar)

资源分配的工作

---

[Mooncake: Kimi’s KVCache-centric Architecture for LLM Serving](https://arxiv.org/abs/2407.00079)

月之暗面的工作

[Mooncake (4): 月饼的皮和馅是怎样制成的，Mooncake 传输引擎开源以及后续的计划 - 知乎](https://zhuanlan.zhihu.com/p/9461861451)

传输部分的优化

---

[Xiaodong Wang](https://www.csl.cornell.edu/~xiaodong/#Publications)

市场理论

高级微观经济学系列：General Equilibrium

贝叶斯优化

---

DLRover

蚂蚁集团论文解析

---

[[2401.11181] Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads](https://arxiv.org/abs/2401.11181)

对于不同的请求的一个优化

---

[Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache](https://arxiv.org/pdf/2401.02669)

alibaba长文本

---

[[2406.17565] MemServe: Context Caching for Disaggregated LLM Serving with Elastic Memory Pool](https://arxiv.org/abs/2406.17565)

华为PD 分离的工作

---

[[2405.07719] USP: A Unified Sequence Parallelism Approach for Long Context Generative AI](https://arxiv.org/abs/2405.07719)

方佳瑞关于长文本的工作，里面包含了带宽需求分析