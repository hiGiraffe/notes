# [ServerlessLLM: Low-Latency Serverless Inference for Large Language Models](https://arxiv.org/abs/2401.14351)

> Yao Fu1 Leyang Xue1 Yeqi Huang1 Andrei-Octavian Brabete1 Dmitrii Ustiugov2 Yuvraj Patel1 Luo Mai1
>
> 1*University of Edinburgh*  2*NTU Singapore*
>
> 爱丁堡Luo Mai老师组的工作，NTU 新AP Dmitrii也有挂名
>
> 原作者的知乎帖子
>
> * [第一视角下关于 ServerlessLLM 的故事（上） - 知乎](https://zhuanlan.zhihu.com/p/710476102)

## 一句话总结概括

加速LLM服务的冷启动

## 背景



## 先前工作存在的问题



## 难点



## 解决方案



## 创新点

1. fast multi-tier checkpoint loading
   * 一种新的checkpoint format
   * a multi-tier loading system
2. efficient live migration of LLM inference
3. startup-time-optimized model scheduling

## 实验评估



## Q&A

