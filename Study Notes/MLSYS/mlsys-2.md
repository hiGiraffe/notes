# è‡ªåŠ¨å¾®åˆ†

* forwardè®¡ç®—å›¾

> ![image-20240417095542434](/images/dl-systems-2/1)

* backwardè®¡ç®—å›¾

> ![image-20240417095813942](/images/dl-systems-2/2)

* åŒæ—¶éœ€è¦è€ƒè™‘åœ¨ä¸åŒé“è·¯ä¸­è¢«ä½¿ç”¨çš„åå‘å¾®åˆ†

> ![image-20240417100014694](/images/dl-systems-2/3)

* åå‘è‡ªåŠ¨å¾®åˆ†ä»£ç 

# å…¨è¿æ¥

> A ğ¿-layer, fully connected network, a.k.a. **multi-layer perceptron (MLP)**, now with an explicit bias term, is defined by the iteration.
>
> ![image-20240417100826417](/images/dl-systems-2/4)
>
> å‚æ•°$\theta=\{W_{1:L},b_{1:L}\}$ï¼Œ$\sigma_{i}$ä¸€èˆ¬æ˜¯éçº¿æ€§çš„æ¿€æ´»ï¼Œä¸€ç§å¸¸ç”¨çš„æ–¹æ³•æ˜¯$\sigma_{L}(x)=x$

# ä¼˜åŒ–å™¨

* æ¢¯åº¦ä¸‹é™æ³•

> ![image-20240417101443854](/images/dl-systems-2/5)
>
> å­¦ä¹ ç‡$\times$æ¢¯åº¦

* Newtonâ€™s Method

> æ ¹æ®Hessianï¼ˆäºŒç»´å¯¼æ•°çŸ©é˜µï¼‰
>
> ![image-20240417102410999](/images/dl-systems-2/6)
>
> ç­‰ä»·äºä½¿ç”¨äºŒé˜¶æ³°å‹’å±•å¼€å°†å‡½æ•°è¿‘ä¼¼ä¸ºäºŒæ¬¡å‡½æ•°ï¼Œç„¶åæ±‚è§£æœ€ä¼˜è§£

* Momentum

> ä¸€ç§è€ƒè™‘æ›´å¤šçš„ä¸­é—´ç»“æ„-momentum updateï¼Œè€ƒè™‘å…ˆå‰æ¢¯åº¦ç§»åŠ¨çš„å¹³å‡å€¼
>
> ![image-20240417103123998](/images/dl-systems-2/7)

* â€œUnbiasingâ€ momentum terms
* Nesterov Momentum
* Adam

> Whether Adam is â€œgoodâ€ optimizer is endlessly debated within deep learning, but it often seems to work quite well in practice (maybe?)

* Stochastic Gradient Descent

# Initialization

åˆå§‹åŒ–è·Ÿå¤§æ¨¡å‹æ¨ç†è²Œä¼¼æ— å…³ï¼Œå°±æ²¡æ·±å…¥å­¦ä¹ äº†

# Normalization 

éœ€è¦çœ‹è§†é¢‘æ‰çœ‹å¾—æ‡‚ï¼Œæ™šç‚¹è¡¥

# Regularization

éœ€è¦çœ‹è§†é¢‘æ‰çœ‹å¾—æ‡‚ï¼Œæ™šç‚¹è¡¥

# Transformer

![image-20240417105236671](/images/dl-systems-2/8)
