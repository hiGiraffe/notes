
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>LLM Deployment Record Â· GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        
        
        
    
    <link rel="stylesheet" href="../../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../../gitbook/gitbook-plugin-back-to-top-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="../../gitbook/gitbook-plugin-chapter-fold/chapter-fold.css">
                
            
                
                <link rel="stylesheet" href="../../gitbook/gitbook-plugin-code/plugin.css">
                
            
                
                <link rel="stylesheet" href="../../gitbook/gitbook-plugin-splitter/splitter.css">
                
            
                
                <link rel="stylesheet" href="../../gitbook/gitbook-plugin-katex/katex.min.css">
                
            
                
                <link rel="stylesheet" href="../../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.css">
                
            
                
                <link rel="stylesheet" href="../../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="Sepculative Decoding.html" />
    
    
    <link rel="prev" href="LLM Calculation.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        <li class="header">Preface</li>
        
        
    
        <li class="chapter " data-level="1.1" data-path="../../">
            
                <a href="../../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Blogs</li>
        
        
    
        <li class="chapter " data-level="2.1" data-path="../../Blogs/Academic Writing/">
            
                <a href="../../Blogs/Academic Writing/">
            
                    
                    Academic Writing
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="2.1.1" data-path="../../Blogs/Academic Writing/design of system.html">
            
                <a href="../../Blogs/Academic Writing/design of system.html">
            
                    
                    Design Of System
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="2.2" data-path="../../Blogs/academic reading.html">
            
                <a href="../../Blogs/academic reading.html">
            
                    
                    Academic Reading
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.3" data-path="../../Blogs/how to express.html">
            
                <a href="../../Blogs/how to express.html">
            
                    
                    How To Express
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Paper Reading Notes</li>
        
        
    
        <li class="chapter " data-level="3.1" data-path="../../Paper Reading Notes/Arxiv/">
            
                <a href="../../Paper Reading Notes/Arxiv/">
            
                    
                    Arxiv
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="3.1.1" data-path="../../Paper Reading Notes/Arxiv/Mooncake.html">
            
                <a href="../../Paper Reading Notes/Arxiv/Mooncake.html">
            
                    
                    Mooncake
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.1.2" data-path="../../Paper Reading Notes/Arxiv/S-LoRA.html">
            
                <a href="../../Paper Reading Notes/Arxiv/S-LoRA.html">
            
                    
                    S-LoRA
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.1.3" data-path="../../Paper Reading Notes/Arxiv/MemServe.md">
            
                <span>
            
                    
                    MemServe
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="3.2" data-path="../../Paper Reading Notes/ASPLOS 2024/">
            
                <a href="../../Paper Reading Notes/ASPLOS 2024/">
            
                    
                    ASPLOS 2024
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="3.2.1" data-path="../../Paper Reading Notes/ASPLOS 2024/ExeGPT.html">
            
                <a href="../../Paper Reading Notes/ASPLOS 2024/ExeGPT.html">
            
                    
                    Exe GPT
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.2.2" data-path="../../Paper Reading Notes/ASPLOS 2024/SpecInfer.html">
            
                <a href="../../Paper Reading Notes/ASPLOS 2024/SpecInfer.html">
            
                    
                    Spec Infer
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="3.3" data-path="../../Paper Reading Notes/OSDI 2024/">
            
                <a href="../../Paper Reading Notes/OSDI 2024/">
            
                    
                    OSDI 2024
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="3.3.1" data-path="../../Paper Reading Notes/OSDI 2024/Llumnix.html">
            
                <a href="../../Paper Reading Notes/OSDI 2024/Llumnix.html">
            
                    
                    Llumnix
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.3.2" data-path="../../Paper Reading Notes/OSDI 2024/ServerlessLLM.html">
            
                <a href="../../Paper Reading Notes/OSDI 2024/ServerlessLLM.html">
            
                    
                    Serverless LLM
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.3.3" data-path="../../Paper Reading Notes/OSDI 2024/Fairness in Serving LLM.md">
            
                <span>
            
                    
                    Fairness in Serving LLM
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="3.4" data-path="../../Paper Reading Notes/SOSP 2024/">
            
                <a href="../../Paper Reading Notes/SOSP 2024/">
            
                    
                    SOSP 2024
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="3.4.1" data-path="../../Paper Reading Notes/SOSP 2024/Apparate.html">
            
                <a href="../../Paper Reading Notes/SOSP 2024/Apparate.html">
            
                    
                    Apparate
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.4.2" data-path="../../Paper Reading Notes/SOSP 2024/LoongServe.html">
            
                <a href="../../Paper Reading Notes/SOSP 2024/LoongServe.html">
            
                    
                    Loong Serve
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="3.5" data-path="../../Paper Reading Notes/FAST 2023/">
            
                <a href="../../Paper Reading Notes/FAST 2023/">
            
                    
                    FAST 2023
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="3.5.1" data-path="../../Paper Reading Notes/FAST 2023/Resource Scheduling for LC Services.html">
            
                <a href="../../Paper Reading Notes/FAST 2023/Resource Scheduling for LC Services.html">
            
                    
                    Resource Scheduling For LC Services
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="3.6" data-path="../../Paper Reading Notes/ICML 2023/">
            
                <a href="../../Paper Reading Notes/ICML 2023/">
            
                    
                    ICML 2023
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="3.6.1" data-path="../../Paper Reading Notes/ICML 2023/FlexGen.html">
            
                <a href="../../Paper Reading Notes/ICML 2023/FlexGen.html">
            
                    
                    Flex Gen
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="3.7" data-path="../../Paper Reading Notes/NSDI 2023/">
            
                <a href="../../Paper Reading Notes/NSDI 2023/">
            
                    
                    NSDI 2023
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="3.7.1" data-path="../../Paper Reading Notes/NSDI 2023/Shockwave.html">
            
                <a href="../../Paper Reading Notes/NSDI 2023/Shockwave.html">
            
                    
                    Shockwave
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="3.8" data-path="../../Paper Reading Notes/SC 2023/">
            
                <a href="../../Paper Reading Notes/SC 2023/">
            
                    
                    SC 2023
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="3.8.1" data-path="../../Paper Reading Notes/SC 2023/AutoMap.html">
            
                <a href="../../Paper Reading Notes/SC 2023/AutoMap.html">
            
                    
                    Auto Map
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="3.9" data-path="../../Paper Reading Notes/SOSP 2023/">
            
                <a href="../../Paper Reading Notes/SOSP 2023/">
            
                    
                    SOSP 2023
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="3.9.1" data-path="../../Paper Reading Notes/SOSP 2023/vllm.html">
            
                <a href="../../Paper Reading Notes/SOSP 2023/vllm.html">
            
                    
                    Vllm
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="3.10" data-path="../../Paper Reading Notes/OSDI 2022/">
            
                <a href="../../Paper Reading Notes/OSDI 2022/">
            
                    
                    OSDI 2022
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="3.10.1" data-path="../../Paper Reading Notes/OSDI 2022/Orca.html">
            
                <a href="../../Paper Reading Notes/OSDI 2022/Orca.html">
            
                    
                    Orca
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="3.11" data-path="../../Paper Reading Notes/SC 2022/">
            
                <a href="../../Paper Reading Notes/SC 2022/">
            
                    
                    SC 2022
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="3.11.1" data-path="../../Paper Reading Notes/SC 2022/CoGNN.html">
            
                <a href="../../Paper Reading Notes/SC 2022/CoGNN.html">
            
                    
                    Co GNN
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.11.2" data-path="../../Paper Reading Notes/SC 2022/VSGM.html">
            
                <a href="../../Paper Reading Notes/SC 2022/VSGM.html">
            
                    
                    VSGM
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="3.12" data-path="../../Paper Reading Notes/OSDI 2020/">
            
                <a href="../../Paper Reading Notes/OSDI 2020/">
            
                    
                    OSDI 2020
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="3.12.1" data-path="../../Paper Reading Notes/OSDI 2020/Gavel.html">
            
                <a href="../../Paper Reading Notes/OSDI 2020/Gavel.html">
            
                    
                    Gavel
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="3.13" data-path="../../Paper Reading Notes/SIGMOD 2020/">
            
                <a href="../../Paper Reading Notes/SIGMOD 2020/">
            
                    
                    SIGMOD 2020
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="3.13.1" data-path="../../Paper Reading Notes/SIGMOD 2020/GPU-based Subgraph Enumerations.html">
            
                <a href="../../Paper Reading Notes/SIGMOD 2020/GPU-based Subgraph Enumerations.html">
            
                    
                    GPU Based Subgraph Enumerations
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="3.14" data-path="../../Paper Reading Notes/OSDI 2018/">
            
                <a href="../../Paper Reading Notes/OSDI 2018/">
            
                    
                    OSDI 2018
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="3.14.1" data-path="../../Paper Reading Notes/OSDI 2018/Ray.html">
            
                <a href="../../Paper Reading Notes/OSDI 2018/Ray.html">
            
                    
                    Ray
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    
        
        <li class="header">Study Notes</li>
        
        
    
        <li class="chapter " data-level="4.1" data-path="../CME 213/">
            
                <a href="../CME 213/">
            
                    
                    CME 213
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="4.1.1" data-path="../CME 213/C++.html">
            
                <a href="../CME 213/C++.html">
            
                    
                    C
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="4.2" data-path="../CUDA/">
            
                <a href="../CUDA/">
            
                    
                    CUDA
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="4.2.1" data-path="../CUDA/CUDA Warp Level.html">
            
                <a href="../CUDA/CUDA Warp Level.html">
            
                    
                    CUDA Warp Level
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.2.2" data-path="../CUDA/CUDA1 Basic.html">
            
                <a href="../CUDA/CUDA1 Basic.html">
            
                    
                    CUDA 1 Basic
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.2.3" data-path="../CUDA/CUDA2 Brief Summary.html">
            
                <a href="../CUDA/CUDA2 Brief Summary.html">
            
                    
                    CUDA 2 Brief Summary
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.2.4" data-path="../CUDA/CUDA3 Kernels.html">
            
                <a href="../CUDA/CUDA3 Kernels.html">
            
                    
                    CUDA 3 Kernels
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.2.5" data-path="../CUDA/Nsight System.html">
            
                <a href="../CUDA/Nsight System.html">
            
                    
                    Nsight System
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="4.3" data-path="../LLM Parallelism/">
            
                <a href="../LLM Parallelism/">
            
                    
                    LLM Parallelism
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="4.3.1" data-path="../LLM Parallelism/Data Parallelism.html">
            
                <a href="../LLM Parallelism/Data Parallelism.html">
            
                    
                    Data Parallelism
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.3.2" data-path="../LLM Parallelism/Pipe Parallelism.html">
            
                <a href="../LLM Parallelism/Pipe Parallelism.html">
            
                    
                    Pipe Parallelism
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.3.3" data-path="../LLM Parallelism/Tensor Parallelism.html">
            
                <a href="../LLM Parallelism/Tensor Parallelism.html">
            
                    
                    Tensor Parallelism
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="4.4" data-path="../MIT 6.172/">
            
                <a href="../MIT 6.172/">
            
                    
                    MIT 6.172
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="4.4.1" data-path="../MIT 6.172/mit-6-172-1.html">
            
                <a href="../MIT 6.172/mit-6-172-1.html">
            
                    
                    Mit 6 172 1
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.4.2" data-path="../MIT 6.172/mit-6-172-12.html">
            
                <a href="../MIT 6.172/mit-6-172-12.html">
            
                    
                    Mit 6 172 12
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.4.3" data-path="../MIT 6.172/mit-6-172-2.html">
            
                <a href="../MIT 6.172/mit-6-172-2.html">
            
                    
                    Mit 6 172 2
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.4.4" data-path="../MIT 6.172/mit-6-172-3.html">
            
                <a href="../MIT 6.172/mit-6-172-3.html">
            
                    
                    Mit 6 172 3
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.4.5" data-path="../MIT 6.172/mit-6-172-hw2.html">
            
                <a href="../MIT 6.172/mit-6-172-hw2.html">
            
                    
                    Mit 6 172 Hw 2
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="4.5" data-path="./">
            
                <a href="./">
            
                    
                    MLSYS
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="4.5.1" data-path="Llama Model's Decoder Code.html">
            
                <a href="Llama Model's Decoder Code.html">
            
                    
                    Llama Model S Decoder Code
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.5.2" data-path="LLM Calculation.html">
            
                <a href="LLM Calculation.html">
            
                    
                    LLM Calculation
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="4.5.3" data-path="LLM Deployment Record.html">
            
                <a href="LLM Deployment Record.html">
            
                    
                    LLM Deployment Record
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.5.4" data-path="Sepculative Decoding.html">
            
                <a href="Sepculative Decoding.html">
            
                    
                    Sepculative Decoding
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="4.6" data-path="../NLP/">
            
                <a href="../NLP/">
            
                    
                    NLP
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="4.6.1" data-path="../NLP/Basics of Machine Learning.html">
            
                <a href="../NLP/Basics of Machine Learning.html">
            
                    
                    Basics Of Machine Learning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.6.2" data-path="../NLP/GPT.html">
            
                <a href="../NLP/GPT.html">
            
                    
                    GPT
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.6.3" data-path="../NLP/Implementation Neural Network.html">
            
                <a href="../NLP/Implementation Neural Network.html">
            
                    
                    Implementation Neural Network
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.6.4" data-path="../NLP/Loss Function.html">
            
                <a href="../NLP/Loss Function.html">
            
                    
                    Loss Function
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.6.5" data-path="../NLP/Recurrent Neural Network.html">
            
                <a href="../NLP/Recurrent Neural Network.html">
            
                    
                    Recurrent Neural Network
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.6.6" data-path="../NLP/Seq2Seq and Attention.html">
            
                <a href="../NLP/Seq2Seq and Attention.html">
            
                    
                    Seq 2 Seq And Attention
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="4.7" data-path="../OPENMP/">
            
                <a href="../OPENMP/">
            
                    
                    OPENMP
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="4.7.1" data-path="../OPENMP/openmp.html">
            
                <a href="../OPENMP/openmp.html">
            
                    
                    OPENMP
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="4.8" data-path="../Triton/">
            
                <a href="../Triton/">
            
                    
                    Triton
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.9" data-path="../Ray/index.md">
            
                <span>
            
                    
                    Ray
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.10" data-path="../vLLM Code/">
            
                <a href="../vLLM Code/">
            
                    
                    VLLM Code
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="4.10.1" data-path="../vLLM Code/vllm-async-llm.html">
            
                <a href="../vLLM Code/vllm-async-llm.html">
            
                    
                    Vllm Async Llm
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.10.2" data-path="../vLLM Code/vllm-attention.html">
            
                <a href="../vLLM Code/vllm-attention.html">
            
                    
                    Vllm Attention
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.10.3" data-path="../vLLM Code/vllm-block.html">
            
                <a href="../vLLM Code/vllm-block.html">
            
                    
                    Vllm Block
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.10.4" data-path="../vLLM Code/vllm-cache.html">
            
                <a href="../vLLM Code/vllm-cache.html">
            
                    
                    Vllm Cache
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.10.5" data-path="../vLLM Code/vllm-chunked-prefill.html">
            
                <a href="../vLLM Code/vllm-chunked-prefill.html">
            
                    
                    Vllm Chunked Prefill
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.10.6" data-path="../vLLM Code/vllm-cpu.html">
            
                <a href="../vLLM Code/vllm-cpu.html">
            
                    
                    Vllm Cpu
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.10.7" data-path="../vLLM Code/vllm-llama.html">
            
                <a href="../vLLM Code/vllm-llama.html">
            
                    
                    Vllm Llama
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.10.8" data-path="../vLLM Code/vllm-LoRA.html">
            
                <a href="../vLLM Code/vllm-LoRA.html">
            
                    
                    Vllm Lo RA
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.10.9" data-path="../vLLM Code/vllm-metadata.html">
            
                <a href="../vLLM Code/vllm-metadata.html">
            
                    
                    Vllm Metadata
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.10.10" data-path="../vLLM Code/vllm-profile.html">
            
                <a href="../vLLM Code/vllm-profile.html">
            
                    
                    Vllm Profile
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.10.11" data-path="../vLLM Code/vllm-ray.html">
            
                <a href="../vLLM Code/vllm-ray.html">
            
                    
                    Vllm Ray
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.10.12" data-path="../vLLM Code/vllm-schedule.html">
            
                <a href="../vLLM Code/vllm-schedule.html">
            
                    
                    Vllm Schedule
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="4.11" data-path="../Llumnix Code/">
            
                <a href="../Llumnix Code/">
            
                    
                    Llumnix Code
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="4.11.1" data-path="../Llumnix Code/llumnix.html">
            
                <a href="../Llumnix Code/llumnix.html">
            
                    
                    Llumnix
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="4.12" data-path="../LoongServe Code/">
            
                <a href="../LoongServe Code/">
            
                    
                    LoongServe Code
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href="../.." >LLM Deployment Record</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="llama2&#x90E8;&#x7F72;&#x8BB0;&#x5F55;">llama2&#x90E8;&#x7F72;&#x8BB0;&#x5F55;</h1>
<p><a href="https://llama.meta.com/llama2/" target="_blank">Llama 2: open source, free for research and commercial use</a></p>
<p><a href="https://huggingface.co/meta-llama" target="_blank">Llama 2 in Hugging Face</a></p>
<p><a href="https://github.com/meta-llama/llama" target="_blank">Llama 2 in Github</a></p>
<p>&#x914D;&#x7F6E;&#x597D;CUDA&#x3001;Pytorch&#xFF0C;&#x4E0B;&#x8F7D;&#x6A21;&#x578B;&#x6570;&#x636E;</p>
<pre><code>pip install -e .
torchrun --nproc_per_node 1 example_text_completion.py \
    --ckpt_dir llama-2-7b/ \
    --tokenizer_path tokenizer.model \
    --max_seq_len 128 --max_batch_size 4
</code></pre><h2 id="vllm&#x90E8;&#x7F72;&#x8BB0;&#x5F55;">VLLM&#x90E8;&#x7F72;&#x8BB0;&#x5F55;</h2>
<p><strong>&#x62A5;&#x9519;</strong></p>
<pre><code>OSError: /home/cjl/llama/llama-2-7b does not appear to have a file named config.json.
</code></pre><p>&#x8C37;&#x6B4C;&#x641C;&#x7D22;&#x5230;&#x662F;&#x56E0;&#x4E3A;weight&#x9700;&#x8981;&#x662F;hf&#x683C;&#x5F0F;&#xFF0C;&#x9700;&#x8981;&#x5229;&#x7528;transformer&#x63D0;&#x4F9B;&#x7684;convert_llama_weights_to_hf.py&#x811A;&#x672C;&#x5C06;&#x5176;&#x53D8;&#x4E3A;hf&#x683C;&#x5F0F;&#x3002;</p>
<p>&#x53C2;&#x8003;<a href="https://huggingface.co/docs/transformers/main/en/model_doc/llama2" target="_blank">transformers llama2 hugging face&#x6587;&#x6863;</a></p>
<pre><code>python src/transformers/models/llama/convert_llama_weights_to_hf.py \
    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path
</code></pre><p>&#x5176;&#x4E2D;&#x5728;&#x672C;&#x673A;&#x4E0A;transformers&#x4F4D;&#x7F6E;</p>
<pre><code>/home/cjl/anaconda3/envs/vllm/lib/python3.9/site-packages/transformers/
</code></pre><pre><code>python /home/cjl/anaconda3/envs/vllm/lib/python3.9/site-packages/transformers/models/llama/convert_llama_weights_to_hf.py  \
    --input_dir /home/cjl/llama/llama-2-7b --model_size 7B --output_dir /home/cjl/llama/llama-2-7b-hf
</code></pre><p><strong>&#x4ECD;&#x7136;&#x62A5;&#x9519;</strong></p>
<pre><code>RuntimeError: [enforce fail at inline_container.cc:424]
</code></pre><p>&#x8C37;&#x6B4C;&#x6CA1;&#x6709;&#x641C;&#x5230;&#x7B54;&#x6848;&#xFF0C;&#x540E;&#x7ECF;&#x9A8C;&#x8BC1;&#x786E;&#x5B9A;&#xFF0C;&#x662F;&#x7531;&#x4E8E;&#x786C;&#x76D8;&#x7A7A;&#x95F4;&#x4E0D;&#x591F;&#x3002;&#x3002;</p>
<p>&#x6E05;&#x7406;&#x7A7A;&#x95F4;&#x540E;&#xFF0C;&#x6210;&#x529F;&#x89E3;&#x51B3;&#x3002;</p>
<p>&#x521B;&#x5EFA;vllm_demo.py&#x6587;&#x4EF6;&#x5E76;&#x8FD0;&#x884C;&#xFF0C;&#x4E00;&#x4E2A;&#x7B80;&#x5355;&#x7684;demo&#x5C31;&#x5B9E;&#x73B0;&#x4E86;&#x3002;</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> vllm <span class="hljs-keyword">import</span> LLM, SamplingParams

prompts = [
    <span class="hljs-string">&quot;Hello, my name is&quot;</span>,
    <span class="hljs-string">&quot;The president of the United States is&quot;</span>,
    <span class="hljs-string">&quot;The capital of France is&quot;</span>,
    <span class="hljs-string">&quot;The future of AI is&quot;</span>,
]
sampling_params = SamplingParams(temperature=<span class="hljs-number">0.8</span>, top_p=<span class="hljs-number">0.95</span>)
llm = LLM(model=<span class="hljs-string">&apos;/home/cjl/llama/llama-2-7b-hf&apos;</span>, dtype=<span class="hljs-string">&apos;half&apos;</span>) 

outputs = llm.generate(prompts, sampling_params)
<span class="hljs-comment"># Print the outputs.</span>
<span class="hljs-keyword">for</span> output <span class="hljs-keyword">in</span> outputs:
    prompt = output.prompt
    generated_text = output.outputs[<span class="hljs-number">0</span>].text
    print(f<span class="hljs-string">&quot;Prompt: {prompt!r}, Generated text: {generated_text!r}&quot;</span>)
</code></pre>
<h3 id="chat-completion&#x529F;&#x80FD;">chat completion&#x529F;&#x80FD;</h3>
<pre><code>curl http://localhost:8000/v1/chat/completions -H &quot;Content-Type: application/json&quot; -d &apos;{
        &quot;model&quot;: &quot;/home/cjl/llama/llama-2-7b-hf&quot;,
        &quot;messages&quot;: [
            {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are an intelligent British female writer and translator who is good at writing science fiction using multiple languages. You won a Nobel price in literature five years ago.&quot;},
            {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Please detailedly tell a story about an exciting aerospace expedition for a Chinese boy Lam and his German dog. They are sent to aerospace by mistake and strive to wait for rescue from motherland with no water and food supply for over a month. They are almost caught by aliens disguised as his mother. Moreover, please translate the above story to Chinese, German, French, Portuguese and Japanese respectively.&quot;}
        ], &quot;temperature&quot;: 0
    }&apos;
</code></pre><h2 id="accelerate&#x90E8;&#x7F72;&#x8BB0;&#x5F55;">Accelerate&#x90E8;&#x7F72;&#x8BB0;&#x5F55;</h2>
<p><a href="https://huggingface.co/docs/accelerate/en/index" target="_blank">Accelerate in Hugging Face</a></p>
<p><a href="https://cloud.tencent.com/developer/news/1257333" target="_blank">Accelerate&#x5355;&#x673A;&#x591A;&#x5361;&#x7B80;&#x5355;demo</a></p>
<p>&#x7B2C;&#x4E00;&#x6B21;&#x4F7F;&#x7528;&#x4E00;&#x4E9B;&#x4F18;&#x5316;&#x8BBE;&#x7F6E;&#xFF0C;&#x4F46;&#x8C8C;&#x4F3C;&#x786C;&#x4EF6;&#x4E0D;&#x592A;&#x9002;&#x914D;&#xFF0C;&#x6240;&#x4EE5;&#x7B2C;&#x4E8C;&#x6B21;&#x8BBE;&#x7F6E;&#x4E86;&#x57FA;&#x672C;&#x4EC0;&#x4E48;&#x4F18;&#x5316;&#x90FD;&#x6CA1;&#x6709;&#x7684;&#x60C5;&#x51B5;&#x3002;</p>
<pre><code> `Accelerate` version: 0.29.0.dev0
- Platform: Linux-5.14.0-362.13.1.el9_3.x86_64-x86_64-with-glibc2.34
- Python version: 3.10.14
- Numpy version: 1.26.4
- PyTorch version (GPU?): 2.2.1+cu121 (True)
- PyTorch XPU available: False
- PyTorch NPU available: False
- PyTorch MLU available: False
- System RAM: 187.06 GB
- GPU type: Tesla V100S-PCIE-32GB
- `Accelerate` default config:
        - compute_environment: LOCAL_MACHINE
        - distributed_type: MULTI_GPU
        - mixed_precision: no
        - use_cpu: False
        - debug: True
        - num_processes: 2
        - machine_rank: 0
        - num_machines: 1
        - gpu_ids: all
        - rdzv_backend: static
        - same_network: True
        - main_training_function: main
        - enable_cpu_affinity: False
        - downcast_bf16: no
        - tpu_use_cluster: False
        - tpu_use_sudo: False
        - tpu_env: []
</code></pre><h2 id="demo-1-&#x7B80;&#x5355;&#x4F7F;&#x7528;">demo 1 &#x7B80;&#x5355;&#x4F7F;&#x7528;</h2>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator

<span class="hljs-keyword">from</span> accelerate.utils <span class="hljs-keyword">import</span> gather_object

accelerator = Accelerator()

<span class="hljs-comment"># each GPU creates a string</span>

message=[ f<span class="hljs-string">&quot;Hello this is GPU {accelerator.process_index}&quot;</span> ] 

<span class="hljs-comment"># collect the messages from all GPUs</span>

messages=gather_object(message)

<span class="hljs-comment"># output the messages only on the main process with accelerator.print() </span>

accelerator.print(messages)
</code></pre>
<p>&#x9700;&#x8981;&#x6CE8;&#x610F;&#x6211;&#x4EEC;&#x8981;&#x91C7;&#x7528;accelerate&#x8FD0;&#x884C;&#xFF0C;&#x800C;&#x4E0D;&#x662F;python&#x8FD0;&#x884C;&#x3002;</p>
<pre><code>$ accelerate launch acc_demo_1.py 
[&apos;Hello this is GPU 0&apos;, &apos;Hello this is GPU 1&apos;]
</code></pre><h2 id="llm-demo">llm demo</h2>
<pre><code class="lang-python">
<span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator

<span class="hljs-keyword">from</span> accelerate.utils <span class="hljs-keyword">import</span> gather_object

<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

<span class="hljs-keyword">from</span> statistics <span class="hljs-keyword">import</span> mean

<span class="hljs-keyword">import</span> torch, time, json

accelerator = Accelerator()

<span class="hljs-comment"># 10*10 Prompts. Source: https://www.penguin.co.uk/articles/2022/04/best-first-lines-in-books</span>

prompts_all=[

    <span class="hljs-string">&quot;The King is dead. Long live the Queen.&quot;</span>,

    <span class="hljs-string">&quot;Once there were four children whose names were Peter, Susan, Edmund, and Lucy.&quot;</span>,

    <span class="hljs-string">&quot;The story so far: in the beginning, the universe was created.&quot;</span>,

    <span class="hljs-string">&quot;It was a bright cold day in April, and the clocks were striking thirteen.&quot;</span>,

    <span class="hljs-string">&quot;It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.&quot;</span>,

    <span class="hljs-string">&quot;The sweat wis lashing oafay Sick Boy; he wis trembling.&quot;</span>,

    <span class="hljs-string">&quot;124 was spiteful. Full of Baby&apos;s venom.&quot;</span>,

    <span class="hljs-string">&quot;As Gregor Samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect.&quot;</span>,

    <span class="hljs-string">&quot;I write this sitting in the kitchen sink.&quot;</span>,

    <span class="hljs-string">&quot;We were somewhere around Barstow on the edge of the desert when the drugs began to take hold.&quot;</span>,

] * <span class="hljs-number">10</span>

<span class="hljs-comment"># load a base model and tokenizer</span>

model_path=<span class="hljs-string">&quot;/home/cjl/llama/llama-2-7b-hf&quot;</span>

model = AutoModelForCausalLM.from_pretrained(

    model_path,   

    device_map={<span class="hljs-string">&quot;&quot;</span>: accelerator.process_index},

    torch_dtype=torch.bfloat16,

)

tokenizer = AutoTokenizer.from_pretrained(model_path)   

<span class="hljs-comment"># sync GPUs and start the timer</span>

accelerator.wait_for_everyone()

start=time.time()

<span class="hljs-comment"># divide the prompt list onto the available GPUs </span>

<span class="hljs-keyword">with</span> accelerator.split_between_processes(prompts_all) <span class="hljs-keyword">as</span> prompts:

    <span class="hljs-comment"># store output of generations in dict</span>

    results=dict(outputs=[], num_tokens=<span class="hljs-number">0</span>)

    <span class="hljs-comment"># have each GPU do inference, prompt by prompt</span>

    <span class="hljs-keyword">for</span> prompt <span class="hljs-keyword">in</span> prompts:

        prompt_tokenized=tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(<span class="hljs-string">&quot;cuda&quot;</span>)

        output_tokenized = model.generate(**prompt_tokenized, max_new_tokens=<span class="hljs-number">100</span>)[<span class="hljs-number">0</span>]

        <span class="hljs-comment"># remove prompt from output </span>

        output_tokenized=output_tokenized[len(prompt_tokenized[<span class="hljs-string">&quot;input_ids&quot;</span>][<span class="hljs-number">0</span>]):]

        <span class="hljs-comment"># store outputs and number of tokens in result{}</span>

        results[<span class="hljs-string">&quot;outputs&quot;</span>].append( tokenizer.decode(output_tokenized) )

        results[<span class="hljs-string">&quot;num_tokens&quot;</span>] += len(output_tokenized)

    results=[ results ] <span class="hljs-comment"># transform to list, otherwise gather_object() will not collect correctly</span>

<span class="hljs-comment"># collect results from all the GPUs</span>

results_gathered=gather_object(results)

<span class="hljs-keyword">if</span> accelerator.is_main_process:

    timediff=time.time()-start

    num_tokens=sum([r[<span class="hljs-string">&quot;num_tokens&quot;</span>] <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> results_gathered ])

    print(f<span class="hljs-string">&quot;tokens/sec: {num_tokens//timediff}, time {timediff}, total tokens {num_tokens}, total prompts {len(prompts_all)}&quot;</span>)

    print(results)
</code></pre>
<pre><code>tokens/sec: 59.0, time 168.46036076545715, total tokens 10000, total prompts 100
</code></pre><h2 id="llm-&#x6279;&#x5904;&#x7406;-demo">llm &#x6279;&#x5904;&#x7406; demo</h2>
<pre><code class="lang-python">
<span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator

<span class="hljs-keyword">from</span> accelerate.utils <span class="hljs-keyword">import</span> gather_object

<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

<span class="hljs-keyword">from</span> statistics <span class="hljs-keyword">import</span> mean

<span class="hljs-keyword">import</span> torch, time, json

accelerator = Accelerator()

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">write_pretty_json</span><span class="hljs-params">(file_path, data)</span>:</span>

    <span class="hljs-keyword">import</span> json

    <span class="hljs-keyword">with</span> open(file_path, <span class="hljs-string">&quot;w&quot;</span>) <span class="hljs-keyword">as</span> write_file:

        json.dump(data, write_file, indent=<span class="hljs-number">4</span>)

<span class="hljs-comment"># 10*10 Prompts. Source: https://www.penguin.co.uk/articles/2022/04/best-first-lines-in-books</span>

prompts_all=[

    <span class="hljs-string">&quot;The King is dead. Long live the Queen.&quot;</span>,

    <span class="hljs-string">&quot;Once there were four children whose names were Peter, Susan, Edmund, and Lucy.&quot;</span>,

    <span class="hljs-string">&quot;The story so far: in the beginning, the universe was created.&quot;</span>,

    <span class="hljs-string">&quot;It was a bright cold day in April, and the clocks were striking thirteen.&quot;</span>,

    <span class="hljs-string">&quot;It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.&quot;</span>,

    <span class="hljs-string">&quot;The sweat wis lashing oafay Sick Boy; he wis trembling.&quot;</span>,

    <span class="hljs-string">&quot;124 was spiteful. Full of Baby&apos;s venom.&quot;</span>,

    <span class="hljs-string">&quot;As Gregor Samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect.&quot;</span>,

    <span class="hljs-string">&quot;I write this sitting in the kitchen sink.&quot;</span>,

    <span class="hljs-string">&quot;We were somewhere around Barstow on the edge of the desert when the drugs began to take hold.&quot;</span>,

] * <span class="hljs-number">10</span>

<span class="hljs-comment"># load a base model and tokenizer</span>

model_path=<span class="hljs-string">&quot;models/llama2-7b&quot;</span>

model = AutoModelForCausalLM.from_pretrained(

    model_path,   

    device_map={<span class="hljs-string">&quot;&quot;</span>: accelerator.process_index},

    torch_dtype=torch.bfloat16,

)

tokenizer = AutoTokenizer.from_pretrained(model_path)   

tokenizer.pad_token = tokenizer.eos_token

<span class="hljs-comment"># batch, left pad (for inference), and tokenize</span>

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">prepare_prompts</span><span class="hljs-params">(prompts, tokenizer, batch_size=<span class="hljs-number">16</span>)</span>:</span>

    batches=[prompts[i:i + batch_size] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, len(prompts), batch_size)] 

    batches_tok=[]

    tokenizer.padding_side=<span class="hljs-string">&quot;left&quot;</span>     

    <span class="hljs-keyword">for</span> prompt_batch <span class="hljs-keyword">in</span> batches:

        batches_tok.append(

            tokenizer(

                prompt_batch, 

                return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, 

                padding=<span class="hljs-string">&apos;longest&apos;</span>, 

                truncation=<span class="hljs-keyword">False</span>, 

                pad_to_multiple_of=<span class="hljs-number">2</span>,

                add_special_tokens=<span class="hljs-keyword">False</span>).to(<span class="hljs-string">&quot;cuda&quot;</span>) 

            )

    tokenizer.padding_side=<span class="hljs-string">&quot;right&quot;</span>

    <span class="hljs-keyword">return</span> batches_tok

<span class="hljs-comment"># sync GPUs and start the timer</span>

accelerator.wait_for_everyone()   

start=time.time()

<span class="hljs-comment"># divide the prompt list onto the available GPUs </span>

<span class="hljs-keyword">with</span> accelerator.split_between_processes(prompts_all) <span class="hljs-keyword">as</span> prompts:

    results=dict(outputs=[], num_tokens=<span class="hljs-number">0</span>)

    <span class="hljs-comment"># have each GPU do inference in batches</span>

    prompt_batches=prepare_prompts(prompts, tokenizer, batch_size=<span class="hljs-number">16</span>)

    <span class="hljs-keyword">for</span> prompts_tokenized <span class="hljs-keyword">in</span> prompt_batches:

        outputs_tokenized=model.generate(**prompts_tokenized, max_new_tokens=<span class="hljs-number">100</span>)

        <span class="hljs-comment"># remove prompt from gen. tokens</span>

        outputs_tokenized=[ tok_out[len(tok_in):] 

            <span class="hljs-keyword">for</span> tok_in, tok_out <span class="hljs-keyword">in</span> zip(prompts_tokenized[<span class="hljs-string">&quot;input_ids&quot;</span>], outputs_tokenized) ] 

        <span class="hljs-comment"># count and decode gen. tokens </span>

        num_tokens=sum([ len(t) <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> outputs_tokenized ])

        outputs=tokenizer.batch_decode(outputs_tokenized)

        <span class="hljs-comment"># store in results{} to be gathered by accelerate</span>

        results[<span class="hljs-string">&quot;outputs&quot;</span>].extend(outputs)

        results[<span class="hljs-string">&quot;num_tokens&quot;</span>] += num_tokens

    results=[ results ] <span class="hljs-comment"># transform to list, otherwise gather_object() will not collect correctly</span>

<span class="hljs-comment"># collect results from all the GPUs</span>

results_gathered=gather_object(results)

<span class="hljs-keyword">if</span> accelerator.is_main_process:

    timediff=time.time()-start

    num_tokens=sum([r[<span class="hljs-string">&quot;num_tokens&quot;</span>] <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> results_gathered ])

    print(f<span class="hljs-string">&quot;tokens/sec: {num_tokens//timediff}, time elapsed: {timediff}, num_tokens {num_tokens}&quot;</span>)
</code></pre>
<pre><code>tokens/sec: 113.0, time elapsed: 87.8477463722229, num_tokens 10000
</code></pre><p><a href="https://huggingface.co/docs/accelerate/main/en/concept_guides/big_model_inference" target="_blank">Accelerate Handling big models for inference</a></p>
<p>&#x4FEE;&#x6539;device_map&#x5B9E;&#x73B0;layer&#x5206;&#x5C42;</p>
<p>&#x4FEE;&#x6539;&#x524D;</p>
<pre><code class="lang-python">model = AutoModelForCausalLM.from_pretrained(

    model_path,   

    device_map={<span class="hljs-string">&quot;&quot;</span>: accelerator.process_index},

    torch_dtype=torch.bfloat16,

)
</code></pre>
<pre><code>Loading checkpoint shards: 100%|&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;| 3/3 [01:08&lt;00:00, 22.69s/it]
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Loading checkpoint shards: 100%|&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;| 3/3 [01:10&lt;00:00, 23.42s/it]
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
tokens/sec: 25.0, time 38.98043131828308, total tokens 1000, total prompts 10
device_map is  {&apos;&apos;: 0}
[{&apos;outputs&apos;: [&apos;\nThe King is dead. Long live the Queen.\nThe King is dead. Long live the King.\nThe King is dead. Long live the King.\nThe King is dead. Long live the King.\nThe King is dead. Long live the King.\nThe King is dead. Long live the King.\nThe King is dead. Long live the King.\nThe King is dead. Long live the King.\nThe King is dead. Long live the King.\n&apos;, &apos;They were sent to the country to stay with their eccentric uncle, who lived in a large house that had been in his family for hundreds of years.\nTheir uncle was a very strange man. He was tall and thin and had a long, hooked nose. He was always dressed in a long black cloak, and he had a long, white beard. He was very fond of children, and he was always telling them stories about the witches who lived in the&apos;, &apos;The universe was without form, and void; and darkness was upon the face of the deep. And the Spirit of God moved upon the face of the waters. And God said, &quot;Let there be light,&quot; and there was light. And God saw the light, that it was good; and God divided the light from the darkness. And God called the light Day, and the darkness he called Night. And the evening and the morning were the first day.\nAnd God said, &quot;Let there&apos;, &apos;\nWinston Smith, his chin nuzzled into his breast in an effort to escape the vile wind, slipped quickly through the glass doors of Victory Mansions, though not quickly enough to prevent a swirl of gritty dust from entering along with him.\nThe hallway smelt of boiled cabbage and old rag mats. At one end of it a colored poster, too large for indoor display, had been tacked to the wall. It&apos;, &apos;\nIt is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife. However little known the feelings or views of such a man may be on his first entering a neighbourhood, this truth is so well fixed in the minds of the surrounding families, that he is considered the rightful property of some one or other of their daughters.\n&quot;It is a truth universally acknowledged, that a single man in possession of a good&apos;], &apos;num_tokens&apos;: 500}]
</code></pre><p>&#x4FEE;&#x6539;&#x540E;</p>
<pre><code>model = AutoModelForCausalLM.from_pretrained(

    model_path,   

    device_map=&quot;auto&quot;,

    torch_dtype=torch.bfloat16,

)
</code></pre><pre><code>Loading checkpoint shards: 100%|&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;| 3/3 [00:06&lt;00:00,  2.21s/it]
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Loading checkpoint shards: 100%|&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;| 3/3 [00:06&lt;00:00,  2.13s/it]
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
tokens/sec: 12.0, time 78.71099257469177, total tokens 1000, total prompts 10
device_map is  {&apos;model.embed_tokens&apos;: 0, &apos;model.layers.0&apos;: 0, &apos;model.layers.1&apos;: 0, &apos;model.layers.2&apos;: 0, &apos;model.layers.3&apos;: 0, &apos;model.layers.4&apos;: 0, &apos;model.layers.5&apos;: 0, &apos;model.layers.6&apos;: 0, &apos;model.layers.7&apos;: 0, &apos;model.layers.8&apos;: 0, &apos;model.layers.9&apos;: 0, &apos;model.layers.10&apos;: 0, &apos;model.layers.11&apos;: 0, &apos;model.layers.12&apos;: 0, &apos;model.layers.13&apos;: 0, &apos;model.layers.14&apos;: 0, &apos;model.layers.15&apos;: 0, &apos;model.layers.16&apos;: 1, &apos;model.layers.17&apos;: 1, &apos;model.layers.18&apos;: 1, &apos;model.layers.19&apos;: 1, &apos;model.layers.20&apos;: 1, &apos;model.layers.21&apos;: 1, &apos;model.layers.22&apos;: 1, &apos;model.layers.23&apos;: 1, &apos;model.layers.24&apos;: 1, &apos;model.layers.25&apos;: 1, &apos;model.layers.26&apos;: 1, &apos;model.layers.27&apos;: 1, &apos;model.layers.28&apos;: 1, &apos;model.layers.29&apos;: 1, &apos;model.layers.30&apos;: 1, &apos;model.layers.31&apos;: 1, &apos;model.norm&apos;: 1, &apos;lm_head&apos;: 1}
[{&apos;outputs&apos;: [&apos;\nThe King is dead. Long live the Queen.\nThe King is dead. Long live the King.\nThe King is dead. Long live the King.\nThe King is dead. Long live the King.\nThe King is dead. Long live the King.\nThe King is dead. Long live the King.\nThe King is dead. Long live the King.\nThe King is dead. Long live the King.\nThe King is dead. Long live the King.\n&apos;, &apos;They were sent to the country to stay with their eccentric uncle, who lived in a large house that had been in his family for hundreds of years.\nTheir uncle was a very strange man. He was tall and thin and had a long, hooked nose. He was always dressed in a long black cloak, and he had a long, white beard. He was very fond of children, and he was always telling them stories about the witches who lived in the&apos;, &apos;The universe was without form, and void; and darkness was upon the face of the deep. And the Spirit of God moved upon the face of the waters. And God said, &quot;Let there be light,&quot; and there was light. And God saw the light, that it was good; and God divided the light from the darkness. And God called the light Day, and the darkness he called Night. And the evening and the morning were the first day.\nAnd God said, &quot;Let there&apos;, &apos;\nWinston Smith, his chin nuzzled into his breast in an effort to escape the vile wind, slipped quickly through the glass doors of Victory Mansions, though not quickly enough to prevent a swirl of gritty dust from entering along with him.\nThe hallway smelt of boiled cabbage and old rag mats. At one end of it a colored poster, too large for indoor display, had been tacked to the wall. It&apos;, &apos;\nIt is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife. However little known the feelings or views of such a man may be on his first entering a neighbourhood, this truth is so well fixed in the minds of the surrounding families, that he is considered the rightful property of some one or other of their daughters.\n&quot;It is a truth universally acknowledged, that a single man in possession of a good&apos;], &apos;num_tokens&apos;: 500}]
</code></pre><h2 id="accelerates-internal-mechanisms">Accelerate&#x2019;s internal mechanisms</h2>
<p><a href="https://huggingface.co/docs/accelerate/main/en/concept_guides/internal_mechanism" target="_blank">Accelerate&#x2019;s internal mechanisms</a></p>
<p>&#x5728;Pytorch&#x4E2D;&#x52A0;&#x8F7D;&#x9884;&#x8BAD;&#x7EC3;&#x6A21;&#x578B;&#x65F6;</p>
<blockquote>
<ol>
<li>Create the model with randomly initialized weights</li>
<li>Load the model weights (in a dictionary usually called a state dict) from the disk</li>
<li>Load those weights inside the model</li>
</ol>
</blockquote>
<h2 id="comparing-performance-between-different-device-setups">Comparing performance between different device setups</h2>
<p><a href="https://huggingface.co/docs/accelerate/main/en/concept_guides/performance" target="_blank">Comparing performance between different device setups</a></p>
<ul>
<li>Setting the right seeds</li>
<li>Observed Batch Sizes</li>
<li>Learning Rates</li>
</ul>
<p>Accelerater&#x7C7B;</p>
<blockquote>
<p>The provided code snippet is a part of the <code>Accelerator</code> class from the Hugging Face <code>accelerate</code> library. The <code>Accelerator</code> class is designed to simplify distributed training and mixed precision training in deep learning applications. It provides a wrapper around various components such as models, optimizers, data loaders, and schedulers, and offers methods to manage these components in a distributed environment.</p>
<p>Here&apos;s a brief overview of some key components and functionalities within the provided code snippet:</p>
<ol>
<li><p><strong>Initialization (<code>__init__</code> method):</strong> The constructor of the <code>Accelerator</code> class takes several arguments related to distributed training, mixed precision, data loading, and logging. It initializes various plugins like DeepSpeed, FullyShardedDataParallel (FSDP), and MegatronLM if they are available and configured.</p>
</li>
<li><p><strong>Attributes:</strong> The class exposes several attributes that provide information about the training setup, such as the device in use, the distributed training configuration, the local and global process indices, and the mixed precision mode.</p>
</li>
<li><p><strong>Prepare Methods:</strong> The <code>prepare</code>, <code>prepare_model</code>, <code>prepare_data_loader</code>, <code>prepare_optimizer</code>, and <code>prepare_scheduler</code> methods are used to wrap the respective objects (model, data loader, optimizer, and scheduler) with the necessary components for distributed training and mixed precision. These methods ensure that the objects are correctly set up to be used in a distributed environment.</p>
</li>
<li><p><strong>Training Utilities:</strong> Methods like <code>backward</code>, <code>unscale_gradients</code>, <code>clip_grad_norm_</code>, and <code>clip_grad_value_</code> are provided to handle gradient scaling, clipping, and reduction in a distributed and mixed-precision context.</p>
</li>
<li><p><strong>Data Handling:</strong> The <code>gather</code>, <code>gather_for_metrics</code>, and <code>pad_across_processes</code> methods are useful for aggregating data across processes and handling unevenly divided datasets in a distributed setting.</p>
</li>
<li><p><strong>Checkpointing:</strong> The <code>save_state</code> and <code>load_state</code> methods allow for saving and loading the state of the training, including models, optimizers, schedulers, and other custom objects. These methods can handle sharding large models and work with different serialization methods.</p>
</li>
<li><p><strong>Memory Management:</strong> The <code>free_memory</code> and <code>clear</code> methods help in releasing references to internal objects and managing memory efficiently during training.</p>
</li>
<li><p><strong>Mixed Precision Training:</strong> The <code>autocast</code> context manager is provided to enable automatic mixed-precision training within a block of code.</p>
</li>
<li><p><strong>Hooks and Callbacks:</strong> The class supports registering hooks for pre-saving and pre-loading states, allowing for custom behavior during checkpointing.</p>
</li>
<li><p><strong>Device Verification:</strong> The <code>verify_device_map</code> method checks if a model has been prepared with a device map that resembles <code>auto</code>, which is not compatible with distributed training.</p>
</li>
</ol>
<p>This class is a powerful tool for researchers and developers working with deep learning models, as it simplifies the process of adapting training code for distributed environments and mixed-precision training. It abstracts away much of the complexity involved in these processes, making it easier to focus on the model architecture and training logic.</p>
</blockquote>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="LLM Calculation.html" class="navigation navigation-prev " aria-label="Previous page: LLM Calculation">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="Sepculative Decoding.html" class="navigation navigation-next " aria-label="Next page: Sepculative Decoding">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"LLM Deployment Record","level":"4.5.3","depth":2,"next":{"title":"Sepculative Decoding","level":"4.5.4","depth":2,"path":"Study Notes/MLSYS/Sepculative Decoding.md","ref":"Study Notes/MLSYS/Sepculative Decoding.md","articles":[]},"previous":{"title":"LLM Calculation","level":"4.5.2","depth":2,"path":"Study Notes/MLSYS/LLM Calculation.md","ref":"Study Notes/MLSYS/LLM Calculation.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":["hide-element","back-to-top-button","chapter-fold","code","splitter","katex","expandable-chapters-small"],"pluginsConfig":{"chapter-fold":{},"splitter":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"code":{"copyButtons":true},"hide-element":{"elements":[".gitbook-link"]},"katex":{},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"back-to-top-button":{},"expandable-chapters-small":{},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"Introduction.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"Study Notes/MLSYS/LLM Deployment Record.md","mtime":"2024-12-02T02:35:31.834Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2024-12-02T02:36:07.078Z"},"basePath":"../..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../../gitbook/gitbook.js"></script>
    <script src="../../gitbook/theme.js"></script>
    
        
        <script src="../../gitbook/gitbook-plugin-hide-element/plugin.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-back-to-top-button/plugin.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-chapter-fold/chapter-fold.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-code/plugin.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-splitter/splitter.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

